<html>
<head>
	<meta charset="utf-8">
    <style>
		body {
		  background: rgb(0,74,134); 
		  background: repeating-linear-gradient(-60deg, rgb(230,230,230) 0,rgb(230,230,230)  1px, transparent 1px, transparent 5px);
   		  counter-reset: page_num ;			
		}
		page {
		  background: white;
		  display: block;
		  margin: .5cm auto;
		  box-shadow: 0 0 0.5cm rgba(0,0,0,0.5);
		}
		page[size="A5"] {  
		  width: 22cm;
		  height: 14.8cm; 
		}
		page[size="A4"] {  
		  width: 21cm;
		  height: 29.7cm; 
		}
		page[size="A4"][layout="portrait"] {
		  width: 29.7cm;
		  height: 21cm;  
		}
		@media print {
		  body, page {
			margin: 0;
			box-shadow: 0;
		  }
		}
		header {			
			padding: 0 0 0 0 ;
			color: white;
			background-color: rgb(0,130,200) ;
			clear: bottom;
			text-align: left;
   		    counter-reset: page_num ;
		}
		
		.header {
			border:1px solid darkgray ;
		}
		.header .image {
			background: url("https://pp.userapi.com/c629212/v629212097/12726/KO-KSydbMF8.jpg") no-repeat;
			width: 100px;
			height: 90px;
			border:0px solid green;
		}
		.header .text {
			font: x-large Times New Roman ;
			border:0px solid blue;
		}
		.header .image, 
		.header .text {
			display: inline-block;
			vertical-align: middle;
		}
		.body {
			margin : 5px ;
		}
		.page_count :
		{
			width: 100px;
			height: 90px;
			content: "page"+counter(page_num) ;
			counter-increment: page_num ;
		}
		.page_count .text {
			border:2px solid white;
			text-align: right;			
			font: x-large Times New Roman ;
		}
	</style>
	<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		TeX: { equationNumbers: { autoNumber: "AMS" } }
		});
	</script>
</head>
<body>

    <page size="A4">
		<header>
			<div class="header">
				<div class="text"><font size="6">Лекция 1.</font><font size="6">Случайные события. Вероятность.</font></div>
			</div>		
		</header>
		<div class="body">
		Рассмотрим дискретное множество \(\Omega\). Каждому элементу \(\omega_i\in\Omega\) поставим в соответствие число 
		\(\p_i\in[0\ldots1]\) таким образом, чтобы \(\sum\limits_{\forall p_i\in\Omega}p_i = 1\).
		\begin{equation}
			H = -K\sum_{i=1}^{n}p_i\log p_i
		  \label{Entropy}
		\end{equation}	
		Для случая когда источник генерирует одно из двух возможных сообщений:
		\begin{equation}
			H = -K(p \log p + q\log q)
		  \label{Entropy2}
		\end{equation}	
		</div>
   </page>
	
    <page size="A4">
		<header>
			<div class="header">
				<div class="text"><font size="6">Лекция 2.</font><font size="6">Энтропия дискретного источника.</font></div>
			</div>		
		</header>
		<div class="body">
	        Рассмотрим два текстовых файла: первый файл содержит произвольные случайные символы.  Второй файл содержит лишь 
	        символы <code>A</code> и <code>B</code> в произвольном порядке. Каждый файл содержит <code>1000</code> символов.
		Каждый символ представляется байтовым значением <code>0..255</code>. Объем второго файла можно значительно сократить,
		если использовать лишь один бит для представления символов <code>A</code> и <code>B</code>. Например, символ <code>A</code>
	        представляется значением <code>0</code>, а символ <code>B</code> - значением <code>1</code>. В этом случае объем второго 
		файла будет равен <code>1000</code> бит. А объем первого файла составляет по-прежнему <code>8000</code> бит. На основании этого 
		примера можно определить количество информации в файле как минимальный размер такого файла позволяющий сохранить всю информацию 
		без потерь. 
	        Рассмотрим второй пример: Файл содержит символы <code>A,B,C,D</code> в произвольном порядке, при этом количество символов
		<code>A-500</code>, количество символов <code>B-480</code>, количество символов <code>С-15</code> и количество символов 
		<code>D-5</code>. Необходимо оперделить количество информации в файле. Так как количество различных символов равно 4, то 
		для их представления требуется 4 бита на символ. В этом случае размер файла составляет <code>4*1000=4000</code> бит. Этот 
		формат представления никак не учитывает, что количество символов <code>A и B</code> значительно превышает 
		количества символов <code>C и D</code>. Вместо поиска оптимального правила для хранения такого набора символов используем
		вероятностное описание:
			
		Энтропия \(H\) дискретного источника может быть представлена в виде(\ref{Entropy}):
		\begin{equation}
			H = -K\sum_{i=1}^{n}p_i\log p_i
		  \label{Entropy2222}
		\end{equation}	
		Для случая когда источник генерирует одно из двух возможных сообщений:
		\begin{equation}
			H = -K(p \log p + q\log q)
		  \label{Entropy223444}
		\end{equation}	
		</div>
		<img src="http://www5a.wolframalpha.com/Calculate/MSP/MSP64881c1beadd87fgid890000641i4befa63ae3e7?MSPStoreType=image/gif&s=52&w=300.&h=183.&cdf=RangeControl" align="middle">

		<div class="footer">1. C. E. Shannon, Bell Syst. Tech. J. 27, 379 (1948). </div>		
   </page>

    <page size="A4">
	    <H1>Энтропия</H1>
		Энтропия \(H(x,y)\) двух источников:
		\begin{equation}
			H(x,y) = -\sum_{i,j}p(i,j)\log p(i,j)
		  \label{Entropyxy}
		\end{equation}	
		\begin{equation}
			H(x) = -\sum_{i,j}p(i,j)\log \sum_{j} p(i,j)
		  \label{Entropy_x}
		\end{equation}	
		\begin{equation}
			H(y) = -\sum_{i,j}p(i,j)\log \sum_{i} p(i,j)
		  \label{Entropy_y}
		\end{equation}	
		И
		\begin{equation}
			H(x,y) \leq H(x)+H(y)
		\end{equation}	
		Равенство \(H(x,y) = H(x)+H(y)\) выполняется когда \(x\) и \(y\) независимы.
    </page>

    <page size="A5">
	    Вероятность того, что переменная \(y\) примет значение \(j\) при условии, что переменная \(x\) приняла значение \(i\):
		\begin{equation}
			p_i(j)=\frac{p(i,j)}{\sum_{j}p(i,j)}
		\end{equation}			
		Условная энтропия \(H_x(y)\) :
		\begin{aligned}
			H_x(y) = -\sum_{i,j}p(i,j)\log p_i(j)=-\sum_{i,j}p(i,j)\log \frac{p(i,j)}{\sum_{j}p(i,j)}= \\
			-\sum_{i,j}p(i,j)\log p(i,j)+\sum_{i,j}p(i,j)\log \sum_{j}p(i,j)=\\
			H(x,y)-H(x)
		\end{aligned}	
	    Таким образом
		\begin{equation}
			H(x,y) = H_x(y) + H(x)
		\end{equation}					
		Условная энтропия показывает меру неопределенности \(y\) при условии, что \(x\) известно.
		\begin{equation}
			H(x)+H(y) \geq H(x,y)=H_x(y) + H(x) \\
		\end{equation}					
		\begin{equation}
			H(y) \geq H_x(y)
		\end{equation}					
    </page>

</body>
</html>
