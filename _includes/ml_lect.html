<html>
<!--https://www.antennahouse.com/CSSInfo/CSS-Page-Tutorial-en.pdf-->
<head>
	<meta charset="utf-8">
    <style>
		body {
		  background: rgb(0,74,134); 
		  background: repeating-linear-gradient(-60deg, rgb(230,230,230) 0,rgb(230,230,230)  1px, transparent 1px, transparent 5px);
         	  font: large Times New Roman ;
		  text-align: justify ;
		}
	        p H1 {
		  padding-left: 	10px ;
		  padding-right: 	10px ;
	        }
		page {
		  background: white;
		  display: block;
		  margin: .5cm auto;
		  box-shadow: 0 0 0.5cm rgba(0,0,0,0.5);
		}
	        page[size="A5"] {  
		  width: 22cm;
		  height: 14.8cm; 
		}
		page[size="A4"] {  
		  width: 21cm;
		  height: 29.7cm; 
		}
		page[size="A4"][layout="portrait"] {
		  width: 29.7cm;
		  height: 21cm;  
		}
		@media print {
		  body, page {
			margin: 0;
			box-shadow: 0;
		  }
		}
		header {			
			padding: 0 0 0 0 ;
			color: white;
			background-color: rgb(0,130,200) ;
			clear: bottom;
			text-align: left;
		}
		
		.header {
			border:1px solid darkgray ;
		}
		.header .image {
			background: url("https://pp.userapi.com/c629212/v629212097/12726/KO-KSydbMF8.jpg") no-repeat;
			width: 100px;
			height: 90px;
			border:0px solid green;
		}
		.header .text {
			font: x-large Times New Roman ;
			border:0px solid blue;
		}
		.header .image, 
		.header .text {
			display: inline-block;
			vertical-align: middle;
		}
		.body {
			margin : 5px ;
		}
	    
	</style>
	<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		TeX: { equationNumbers: { autoNumber: "AMS" } }
		});
	</script>
</head>
<body>

    <page size="A4">
	<header>
		<div class="header">
			<div class="text"><font size="6">Лекция 1.</font><font size="6">Общая постановка задачи машинного обучения.</font></div>
		</div>		
	</header>
	<div class="body">
	<p>
	<H1>Общая постановка задачи машинного обучения</H1> 
	</p>
	<p>
	Будем говорить, что алгоритм \(A\) решает задачу машинного обучения \(T\) на основании опыта \(E\) в смысле критерия качества \(P\), если 
	с ростом количества опыта \(E\) качество \(P\) решения задачи \(T\) растет [<a href="#Goodfellow">Goodfellow</a>].
	</p>
	<p>
	Среди наиболее распространенных задач \(T\) машинного обучения можно выделить следующие:
	<ul>
	  <li>Классификация.</li>
	  <li>Регрессия.</li>
	  <li>Кластеризация (оценка функции распределения).</li>
	</ul>
	</p>
	<p>
	Под опытом \(E\) в большинстве случаев понимается набор данных (dataset) определяющий для каждого экземпляра данных набор признаков.
	В случае обучения с учителем каждому экземпляру данных ставится в соответсвие метка (label, target). В случае обучения без 
	учителя метка не задается. Для некоторых алгоритмов вместо фиксированного набора данных используется принцип взаимодействия 
	алгоритма обучения с окружающей средой (reinforcement learning).
	</p>
	<p>
	Выбор критерия качества \(P\) определяется видом задачи \(T\). Для задачи классификации \(P\) может быть определено как вероятность
	(не)правильной классификации. Для регрессии в качестве критерия \(P\) часто используется дисперсия ошибки предсказания.
	</p>
	<p>
	Важным отличием задачи машинного обучения от задачи оптимизации является требование обеспечения заданного  
	качества \(P\) для данных на которых обучение не проводилось. Для проверки этого требования набор данных \(E\) разделяют 
	на обучающую выборку \(E_{train}\) (training dataset), на которых проводят обучение алгоритма, и тестовые данные \(E_{test}\) (test dataset), которые при обучении не используются. 
	Если алгоритм \(A\) демонстрирует высокое качество \(P_{Train}\) на тренировочном наборе данных и низкое качество \(P_{Test}\) на тестовом наборе данных, то говорят о переобучении алгоритма \(A\).
	Способность алгоритма \(A\) достигать одинакового качества на тестовых и тренировочных данных называется генерализацией.
	</p>
	<H1>Примеры вопросов.</H1> 
	<p>
	<ol>
	  <li>Приведите примеры задач машинного обучения.</li>
	  <li>Что означает термин переобучение?</li>
	  <li>Что означает термин генерализация?</li>
	</ol>			
	</p>
	<H1>Список литературы.</H1> 
	<p>
	<ol>
	  <li><a name="Goodfellow"></a>  Гудфеллоу Я., Бенджио И., Курвилль А. Глубокое обучение. ДМК Пресс. 2017 </li>
	</ol>			
	</p>
	</div>
   </page>
	
    <page size="A4">
	<header>
		<div class="header">
			<div class="text"><font size="6">Лекция 2.</font><font size="6">Байесовский классификатор.</font></div>
		</div>		
	</header>
	<div class="body">
	<H1>Задача классификации.</H1> 
	<p>
	Задача классификации состоит в предсказании ненаблюдаемой дискретной величины \(y_i\), обозначающей номер класса, на основании 
	наблюдаемого \(i\)-го набора признаков \(\vec{x}_i\). В качестве критерия качества классификации \(P_{class}\) 
	будем использовать частоту ошибочной классификации [<a href="#ISL1">ISL1</a>]:
	\begin{equation}
		P_{class}=\frac{1}{n}\sum\limits_{i=1}^{n}I\left(y_i-\hat{y}_i\right)
		\label{pclass}
	 \end{equation}		
	Здесь \(\hat{y}_i\) - предсказанный класс для \(i\)-го набора признаков. \({y}_i\) - класс для которого был получен набор признаков.
	\(I\) - индикаторная функция равная нулю для нулевого аргумента, либо 1 для всех остальных значений.
	</p>
	<H1>Байесовский классификатор.</H1> 
	<p>
	Можно доказать, что для минимизации критерия (\ref{pclass}) следует выбирать такой номер класса \(k\), для которого
	условная вероятность \(p_k\left(y_i=k|\vec{x_i}\right)\) была бы максимальной среди всех условных вероятностей \(p_k, k\in 1\ldots K\) для наблюдаемого вектора \(\vec{x_i}\) [<a href="#ISL1">ISL1</a>].
	Такой алгоритм классификации называется байесовским классификатором.
	</p>
	<p>
	На практике оценивать \(p_k\left(y_i=k|\vec{x_i}\right)\) по обучающей выборке неудобно [<a href="#CourseraBayesClassifier">Coursera1</a>]. Вместо этого оценивают
	\(p_i\left(\vec{x_i}|y_i=k\right)\). Затем, пользуясь формулой Байеса, получают вероятность \(p_k\).
	\begin{equation}
		p_k\left(y_i=k|\vec{x_i}\right) = \frac{p_i\left(\vec{x_i}|y_i=k\right)p(y_i=k)}{p(\vec{x_i})} 
		\label{Bayes1}
	 \end{equation}		
	Вероятность \(p(y_i=k)\) называют априорной или доопытной, так как она известна до получения вектора \(\vec{x_i}\).
	Условную вероятность \(p_k\left(y_i=k|\vec{x_i}\right)\) называют апостериорной или постопытной, так как она уточняет 
	априорное значение \(p(y_i=k)\) после получения вектора признаков \(\vec{x_i}\).
	</p>
	<H1>Примеры вопросов.</H1> 
	<p>
	<ol>
	  <li>Что означает термин априорная вероятность?</li>
	  <li>Что означает термин апостериорная вероятность?</li>
	</ol>			
	</p>
	<H1>Список литературы.</H1> 
	<p>
	<ol>
	  <li><a name="ISL1"></a> 				<a href="http://www-bcf.usc.edu/~gareth/ISL/">Trevor Hastie, Robert Tibshirani et al. An Introduction to Statistical Learning with Applications in R. стр. 37.</a> </li>
	  <li><a name="CourseraBayesClassifier"></a> 		<a href="https://ru.coursera.org/lecture/supervised-learning/baiiesovskaia-klassifikatsiia-sodierzhaniie-uroka-6Qh0L">Байесовская классификация. - Coursera. Машинное обучение и анализ данных. </a> </li>
	  <li><a name="CourseraNaiveBayesClassifier"></a> 	<a href="https://ru.coursera.org/lecture/supervised-learning/spam-fil-try-i-naivnyi-baiiesovskii-klassifikator-8RMf8">Спам-фильтры и наивный байесовский классификатор. - Coursera. Машинное обучение и анализ данных. </a> </li>
	</ol>			
	</p>
	</div>
	
   </page>

    <page size="A4">
	<header>
		<div class="header">
			<div class="text"><font size="6">Практическое занятие 1.</font><font size="6">Классификация многомерных гауссовых величин.</font></div>
		</div>		
	</header>
	<div class="body">
	<H1>Задача.</H1> 
	<p>
	Рассмотрим набор данных \(E\) состоящий из случайных векторов \(\vec{x_i}\), \(i\in 1\ldots n\)
	\begin{equation}
	  \vec{x_i} = 
	    \begin{pmatrix}
		x_{1}(i) \\
		x_{2}(i) \\
		\ldots \\
		x_{D}(i)
	    \end{pmatrix}
	  \label{RandomVector2}	
	\end{equation}	
	распределение которых задано с помощью многомерной гауссовой плотности:
	\begin{equation}
		p(\vec{x_i},k)=\frac{1}{(2\pi)^\frac{D}{2}|\Sigma_k|^\frac{1}{2}}\exp\left[-\frac{1}{2}\left(\vec{x}-\vec{\mu_k}\right)^H\Sigma_k^{-1}\left(\vec{x}-\vec{\mu_k}\right)\right]
		\label{vgauss}
	 \end{equation}
	Каждому вектору \(x_i\) поставлено в соответствие целое число (метка, label, target) \(k\in 1,\ldots K\), обозначающее номер класса, которое определяет вектор математического 
	ожидания \(\mu_k\) и корреляционную матрицу \(\Sigma_k\) для \(k\)-го класса. Необходимо реализовать байесовский классификатор 
	для такого набора данных. Априорную вероятность принадлежности вектора \(x_i\) к классу \(k\) считать одинаковой для всех классов 
	\(p(y_i=k)=\frac{1}{K}\). Количество классов \(K=2\). 
	</p>
	<H1>Решение.</H1> 
	<p>
	Условная вероятность 
	\begin{equation}
		p_k\left(y_i=k|\vec{x_i}\right) = \frac{p_i\left(\vec{x_i}|y_i=k\right)p(y_i=k)}{p(\vec{x_i})} = \frac{p\left(\vec{x_i},k\right)\frac{1}{K}}{\sum\limits_{m=1}^{K}p(\vec{x_i},m)}
		\label{BayesExs1}
	 \end{equation}		
	Знаменатель в (\ref{BayesExs1}) не зависит от номера класса \(k\). Числитель домноженный на общий для всех классов коэффициент \((2\pi)^\frac{D}{2}\) после логарифмирования:
	\begin{equation}
		\phi_k(\vec{x_i})=\ln\left((2\pi)^\frac{D}{2}p(\vec{x_i},k)\right)=\frac{1}{|\Sigma_k|^\frac{1}{2}}\left(-\frac{1}{2}\left(\vec{x}-\vec{\mu_k}\right)^H\Sigma_k^{-1}\left(\vec{x}-\vec{\mu_k}\right)\right)
		\label{LogNum1}
	 \end{equation}		
	Логарифм является монотонной функцией, поэтому выражение (\ref{LogNum1}) можно использовать вместо (\ref{BayesExs1}) для классификации.
	Процедура байесовской классификации вектора \(\vec{x_i}\) в этом случае сводится к выбору такого класса \(k^*\), для которого значение функции 
	\(\phi_{k^*}(\vec{x_i})\) будет максимальным среди всех остальных \(\phi_{k}(\vec{x_i})\) \(\forany k\neq k^*\).
	\begin{equation}
		k^* = \arg\max_{k\in 1\ldots K}\left(\phi_k(\vec{x_i})\right)
		\label{Fisher1}
	 \end{equation}		
	</p>
	<H1>Список литературы.</H1> 
	<p>
	<ol>
	  <li><a name="Bolshakov"></a> 				<a href="http://www.ozon.ru/context/detail/id/3001036/">Большаков А. Каримов Р. Методы обработки многомерных данных и временных рядов. - Горячая Линия-Телеком, 2007.</a> </li>
	  <li><a name="CourseraNaiveBayesClassifier"></a> 	<a href="https://ru.coursera.org/lecture/supervised-learning/spam-fil-try-i-naivnyi-baiiesovskii-klassifikator-8RMf8">Спам-фильтры и наивный байесовский классификатор. - Coursera. Машинное обучение и анализ данных. </a> </li>
	</ol>			
	</p>
	</div>
	
   </page>

    <page size="A4">
	<header>
		<div class="header">
			<div class="text"><font size="6">Лекция 3.</font><font size="6">Метод максимума правдоподобия.</font></div>
		</div>		
	</header>
	<div class="body">
	<p>
	Метод максимального правдоподобия определяет способ оценки неизвестного параметра \(x\) на основе имеющихся наблюдений \(\vec{y}\) и 
	совместной вероятности \(p(x,\vec{y})\). В качестве оценки максимального правдоподобия принимается значение максимизирующее вероятность
	\(p(x,\vec{y})\) для заданных наблюдений \(\vec{y}\).
	\begin{equation}
		\hat{x} = \arg \max_{x}\left( p(x,\vec{y}) \right)
	  \label{MaximumLikelihood}
	\end{equation}	
	В роли \(x\) может выступать неслучайная величина, например, параметр распределения: математическое ожидание или дисперсия.
	</p>
	
	<H1>Список литературы.</H1> 
	<p>
	<ol>
	  <li><a name="ISL1"></a> 				<a href="http://www-bcf.usc.edu/~gareth/ISL/">Trevor Hastie, Robert Tibshirani et al. An Introduction to Statistical Learning with Applications in R. стр. 37.</a> </li>
	  <li><a name="CourseraBayesClassifier"></a> 		<a href="https://ru.coursera.org/lecture/supervised-learning/baiiesovskaia-klassifikatsiia-sodierzhaniie-uroka-6Qh0L">Байесовская классификация. - Coursera. Машинное обучение и анализ данных. </a> </li>
	  <li><a name="CourseraNaiveBayesClassifier"></a> 	<a href="https://ru.coursera.org/lecture/supervised-learning/spam-fil-try-i-naivnyi-baiiesovskii-klassifikator-8RMf8">Спам-фильтры и наивный байесовский классификатор. - Coursera. Машинное обучение и анализ данных. </a> </li>
	</ol>			
	</p>
	</div>
	
   </page>


</body>
</html>
